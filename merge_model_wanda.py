import os
# ğŸ” Redirect all Hugging Face-related caches and locks to your personal directory
os.environ["HF_HOME"] = "/data/songyao/.cache/huggingface"
os.environ["TRANSFORMERS_CACHE"] = "/data/songyao/.cache/huggingface/transformers"
os.environ["HF_DATASETS_CACHE"] = "/data/songyao/.cache/huggingface/datasets"
os.environ["HF_HUB_CACHE"] = "/data/songyao/.cache/huggingface/hub"



from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
from utils.load_config import cache_dir
from utils.utils import set_random_seed, smart_tokenizer_and_embedding_resize
import numpy as np
from record_activations import load_model_and_tokenizer








def calc_delta_weights_from_models(finetune_model, base_model, verbose=True):
    """
    è®¡ç®— finetune_model ä¸ base_model çš„å‚æ•°å·®ï¼ˆdeltaï¼‰ï¼Œä»¥ state_dict å½¢å¼è¿”å›ã€‚
    è¦æ±‚ä¸¤ä¸ªæ¨¡å‹å®Œå…¨ shape å¯¹é½ã€‚
    è¿”å›: delta_state_dict
    """
    finetune_sd = finetune_model.state_dict()
    base_sd = base_model.state_dict()
    delta_sd = {}

    all_keys = set(finetune_sd.keys()) & set(base_sd.keys())
    extra_keys_finetune = set(finetune_sd.keys()) - set(base_sd.keys())
    extra_keys_base = set(base_sd.keys()) - set(finetune_sd.keys())

    if verbose:
        if extra_keys_finetune:
            print(f"[è­¦å‘Š] finetune æ¨¡å‹æ¯” base å¤šå‡ºå‚æ•°: {extra_keys_finetune}")
        if extra_keys_base:
            print(f"[è­¦å‘Š] base æ¨¡å‹æ¯” finetune å¤šå‡ºå‚æ•°: {extra_keys_base}")

    for k in all_keys:
        v_finetune = finetune_sd[k]
        v_base = base_sd[k]
        if v_finetune.shape != v_base.shape:
            raise ValueError(f"å‚æ•° {k} ç»´åº¦ä¸ä¸€è‡´: finetune {v_finetune.shape}, base {v_base.shape}")
        delta = v_finetune.detach() - v_base.detach()
        delta_sd[k] = delta
        # å¯é€‰ï¼šå¯¹çº¯ embedding int å‚æ•°ç­‰ä¸åšdelta
        # if v_finetune.dtype not in [torch.float16, torch.float32, torch.bfloat16]:
        #     delta_sd[k] = v_finetune.detach().clone()

    return delta_sd



def load_wanda_metrics(npz_path, verbose=True):
    """
    åŠ è½½ wanda æŒ‡æ ‡ npz æ–‡ä»¶ï¼Œè¿”å› {module_name: wanda_matrix} å­—å…¸
    å‚æ•°:
        npz_path: wanda æŒ‡æ ‡ä¿å­˜è·¯å¾„ï¼ˆå¦‚ .../wizardmath_wanda_metrics.npzï¼‰
    è¿”å›:
        wanda_dict: {str: np.ndarray}
    """
    wanda_data = np.load(npz_path, allow_pickle=True)
    wanda_dict = {k: wanda_data[k] for k in wanda_data.files}
    if verbose:
        print(f"Loaded wanda metrics from {npz_path}: {list(wanda_dict.keys())}")
    return wanda_dict



def merge_delta_weight_wise_normalized(
    delta1, delta2, wanda1, wanda2, merge_strategy="softmax", eps=1e-6, amplification_factor = 1
):
    """
    ç”¨å½’ä¸€åŒ–ç›¸å¯¹é‡è¦æ€§ (4*(w1-w2)/(w1+w2+eps)) ä½œä¸º sigmoid è¾“å…¥
    """
    if not isinstance(delta1, torch.Tensor):
        delta1 = torch.from_numpy(delta1)
    if not isinstance(delta2, torch.Tensor):
        delta2 = torch.from_numpy(delta2)
    if not isinstance(wanda1, torch.Tensor):
        wanda1 = torch.from_numpy(wanda1)
    if not isinstance(wanda2, torch.Tensor):
        wanda2 = torch.from_numpy(wanda2)
    assert delta1.shape == delta2.shape == wanda1.shape == wanda2.shape

    # s = 4.0 * (wanda1 - wanda2) / (wanda1 + wanda2 + eps)
    s = 4.0 * (wanda1 - wanda2)
    # å…¶ä»–è¿˜æœ‰ â€œæ¬§æ°è·ç¦»å½’ä¸€åŒ–â€æ–¹å¼
    if merge_strategy == "hard":
        mask = wanda1 > wanda2
        alpha = mask.float()
    elif merge_strategy == "softmax":
        alpha = torch.sigmoid(s)
    merged_delta = amplification_factor*(alpha * delta1 + (1.0 - alpha) * delta2)
    return merged_delta



def merge_arithmetic_delta(delta1, delta2, alpha=0.5):
    """
    å¯¹ä¸€ç»´å‚æ•° delta åš task arithmetic èåˆ
    delta1, delta2: torch.Tensor æˆ– np.ndarrayï¼Œå½¢çŠ¶å¿…é¡»ç›¸åŒä¸”ä¸º1ç»´
    alpha: floatï¼Œèåˆç³»æ•°ï¼Œalpha=1åªç”¨delta1ï¼Œ0åªç”¨delta2ï¼Œ0.5ç­‰æƒå¹³å‡
    è¿”å›: merged_delta
    """
    if not isinstance(delta1, torch.Tensor):
        delta1 = torch.from_numpy(delta1)
    if not isinstance(delta2, torch.Tensor):
        delta2 = torch.from_numpy(delta2)
    assert delta1.shape == delta2.shape
    # assert delta1.ndim == 1, "åªèƒ½ç”¨äºä¸€ç»´å‚æ•°"
    # merged_delta = alpha * delta1 + (1.0 - alpha) * delta2
    merged_delta = alpha * delta1 + alpha * delta2
    return merged_delta





def assemble_merged_model(base_model, merged_delta_dict, verbose=True):
    """
    è¾“å…¥:
        base_model: transformersåŠ è½½çš„æ¨¡å‹ æˆ– base_model.state_dict()
        merged_delta_dict: {å‚æ•°å: deltaå¼ é‡}ï¼Œä¸base_modelå‚æ•°åŒååŒshape
    è¾“å‡º:
        merged_state_dict: æ–°æƒé‡dictï¼Œå¯ç›´æ¥ç”¨äº save/load
    """
    # æ”¯æŒ base_model ä¸ºæ¨¡å‹å¯¹è±¡æˆ– state_dict
    if hasattr(base_model, "state_dict"):
        base_sd = base_model.state_dict()
    else:
        base_sd = base_model  # å·²æ˜¯ state_dict

    merged_state_dict = {}
    mismatch_keys = []
    for k in base_sd:
        if k in merged_delta_dict:
            try:
                merged_state_dict[k] = base_sd[k] + merged_delta_dict[k].to(base_sd[k].device, dtype=base_sd[k].dtype)
            except Exception as e:
                mismatch_keys.append((k, base_sd[k].shape, merged_delta_dict[k].shape))
                if verbose:
                    print(f"è­¦å‘Š: å‚æ•° {k} shape ä¸ä¸€è‡´, base {base_sd[k].shape}, delta {merged_delta_dict[k].shape}")
                # ç›´æ¥ç”¨baseå‚æ•°
                merged_state_dict[k] = base_sd[k]
        else:
            merged_state_dict[k] = base_sd[k]  # æ²¡æœ‰deltaçš„å‚æ•°ç”¨base
            print(f"è­¦å‘Š: åœ¨delta_dictä¸­æœªæ‰¾åˆ°å‚æ•°{k}")

    if verbose and mismatch_keys:
        print(f"[è­¦å‘Š] ä»¥ä¸‹å‚æ•°shapeä¸åŒ¹é…, å·²ç”¨baseå‚æ•°: {mismatch_keys}")

    return merged_state_dict






def save_state_dict(state_dict, out_path, verbose=True):
    """
    ä¿å­˜ PyTorch state_dict åˆ°æœ¬åœ°æ–‡ä»¶.
    å‚æ•°:
        state_dict: dictï¼Œæ¨¡å‹å‚æ•°
        out_path: ä¿å­˜æ–‡ä»¶å (å¦‚ 'merged_model.pth')
    """
    out_dir = os.path.dirname(out_path)
    if out_dir and not os.path.exists(out_dir):
        os.makedirs(out_dir, exist_ok=True)
    torch.save(state_dict, out_path)
    if verbose:
        print(f"å·²ä¿å­˜æ–°æƒé‡åˆ°: {out_path}")


def save_hf_pretrained(base_model, base_tokenizer, merged_state_dict, hf_save_dir):
    base_model.load_state_dict(merged_state_dict)
    base_model.save_pretrained(hf_save_dir)
    base_tokenizer.save_pretrained(hf_save_dir)
    print(f"èåˆæ¨¡å‹å·²ä¿å­˜ä¸º HuggingFace ç›®å½•: {hf_save_dir}")



def find_wanda_key(k, wanda_dict):
    # ä¼˜å…ˆç²¾ç¡®æŸ¥æ‰¾
    if k in wanda_dict:
        return k
    # å¦‚æœkæ˜¯xx.weightç»“å°¾ï¼Œè€Œwanda_dictæ˜¯æ²¡.weightçš„ï¼ŒstripåæŸ¥æ‰¾
    if k.endswith('.weight'):
        k_no_weight = k[:-7]
        if k_no_weight in wanda_dict:
            return k_no_weight
    return None

def run_model_merging_with_wanda(
    base_model_name,
    math_model_name,
    code_model_name,
    wanda_math_path,
    wanda_code_path,
    out_path,
    merge_strategy="softmax",    # "softmax" or "hard"
    norm_param_names=("norm", "layernorm", "ln"),  # ç”¨äºä¸€ç»´å‚æ•°åˆ¤æ–­
    alpha_norm=0.5,           # normå±‚èåˆç³»æ•°
    amplification_factor = 1,
    verbose=True,
):
    """
    ç”¨äºç²¾ç»†åŒ–èåˆ WizardMath-13B (Math) å’Œ llama-2-13b-codealpaca (Code) ä¸¤æ¨¡å‹
    """
    # ===== 1. åŠ è½½æ¨¡å‹ =====
    print("åŠ è½½ base, math, code ä¸‰ä¸ªæ¨¡å‹ ...")
    base_model, base_tokenizer = load_model_and_tokenizer(base_model_name, half_model_dtype=False, seed=0)
    math_model, _ = load_model_and_tokenizer(math_model_name, half_model_dtype=False, seed=0)
    code_model, _ = load_model_and_tokenizer(code_model_name, half_model_dtype=False, seed=0)

    # ===== 2. è®¡ç®— delta =====
    print("è®¡ç®— delta weights ...")
    delta_math = calc_delta_weights_from_models(math_model, base_model)
    delta_code = calc_delta_weights_from_models(code_model, base_model)

    # ===== 3. åŠ è½½ wanda æŒ‡æ ‡ =====
    print("åŠ è½½ wanda æŒ‡æ ‡ ...")
    wanda_math = load_wanda_metrics(wanda_math_path)
    wanda_code = load_wanda_metrics(wanda_code_path)

    # ===== 4. èåˆ delta =====
    print("å¼€å§‹èåˆ delta ...")
    merged_delta = {}
    for k in delta_math:
        v1, v2 = delta_math[k], delta_code[k]
        k_wanda1 = find_wanda_key(k, wanda_math)
        k_wanda2 = find_wanda_key(k, wanda_code)
        
        # çº¿æ€§/MLPå‚æ•°ï¼ˆæƒé‡ï¼‰ç”¨ weight-wise wanda èåˆ
        if (k_wanda1 is not None) and (k_wanda2 is not None) and (v1.ndim == 2):
            assert v1.shape == v2.shape == wanda_math[k_wanda1].shape == wanda_code[k_wanda2].shape
            merged_delta[k] = merge_delta_weight_wise_normalized(
                v1, v2, wanda_math[k_wanda1], wanda_code[k_wanda2], merge_strategy=merge_strategy, amplification_factor = amplification_factor
            )
        # ä¸€ç»´å‚æ•°ï¼ˆnorm, bias ç­‰ï¼‰
        elif any(sub in k.lower() for sub in norm_param_names) and v1.ndim == 1:
            merged_delta[k] = merge_arithmetic_delta(v1, v2, alpha=alpha_norm)
        # 3. embed_tokens.weight ç‰¹åˆ¤
        elif k.endswith("embed_tokens.weight") and v1.ndim == 2:
            merged_delta[k] = merge_arithmetic_delta(v1, v2, alpha=0.5)  # ç­‰æƒå¹³å‡
        else:
            # fallback: å‡å€¼
            merged_delta[k] = (v1 + v2) / 2
            if verbose:
                print(f"æ³¨æ„: {k} æœªç”¨ wanda èåˆ, ç›´æ¥å‡å€¼.")

    # ===== 5. åˆæˆæ–°æ¨¡å‹æƒé‡ =====
    print("åˆæˆæ–°æ¨¡å‹æƒé‡ ...")
    merged_state_dict = assemble_merged_model(base_model, merged_delta)

    # ===== 6. ä¿å­˜ =====
    save_state_dict(merged_state_dict, out_path)
    print(f"å®Œæˆï¼èåˆæ¨¡å‹å·²ä¿å­˜åˆ° {out_path}")
    
    hf_save_dir = out_path.replace('.pth', '_hf')
    save_hf_pretrained(base_model, base_tokenizer, merged_state_dict, hf_save_dir)


if __name__ == "__main__":
    run_model_merging_with_wanda(
        base_model_name="Llama-2-13b-hf",
        math_model_name="WizardMath-13B-V1.0",
        code_model_name="llama-2-13b-code-alpaca",
        wanda_math_path="activations/gsm8k/n64/WizardMath-13B-V1.0_wanda_metrics.npz",
        wanda_code_path="activations/humaneval/n64/llama-2-13b-code-alpaca_wanda_metrics.npz",
        out_path="merged_models/llama-2-13b-math-code-wanda-merge_softmax_amplification_factor_2_alpha_norm_1.pth",
        merge_strategy="softmax",      # æˆ– "hard"
        alpha_norm=1,             # norm/bias ç­‰ä¸€ç»´å‚æ•°ç”¨ç­‰æƒ
        amplification_factor = 2,
        verbose=True,
    )

