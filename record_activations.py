import os
# ğŸ” Redirect all Hugging Face-related caches and locks to your personal directory
os.environ["HF_HOME"] = "/data/songyao/.cache/huggingface"
os.environ["TRANSFORMERS_CACHE"] = "/data/songyao/.cache/huggingface/transformers"
os.environ["HF_DATASETS_CACHE"] = "/data/songyao/.cache/huggingface/datasets"
os.environ["HF_HUB_CACHE"] = "/data/songyao/.cache/huggingface/hub"

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
from utils.load_config import cache_dir
from utils.utils import set_random_seed, smart_tokenizer_and_embedding_resize
import torch.nn as nn
import numpy as np
import os
import gc
from datasets import load_dataset


def load_model_and_tokenizer(model_name, half_model_dtype=False, seed=0, device ="cpu"):
    """
    åŠ è½½æŒ‡å®š HuggingFace æ ¼å¼çš„ LLM æ¨¡å‹å’Œåˆ†è¯å™¨ï¼Œå¹¶è‡ªåŠ¨æ”¾åˆ° GPUã€‚
    æ”¯æŒ fp16ï¼Œé€‚é…å¤§éƒ¨åˆ† LLaMA/CodeAlpaca/WizardMath ç­‰æ¨¡å‹ã€‚

    å‚æ•°:
        model_name: str, HuggingFace hub ä¸Šçš„æ¨¡å‹åæˆ–æœ¬åœ°è·¯å¾„

    è¿”å›:
        model: åŠ è½½å¥½çš„æ¨¡å‹ï¼ˆå·²æ”¾åˆ° deviceï¼‰
        tokenizer: åŠ è½½å¥½çš„åˆ†è¯å™¨
    """
    print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_name}")
    
    # 1. åŠ è½½æ¨¡å‹å’Œ tokenizerï¼Œæ”¯æŒæœ¬åœ°å’Œè¿œç¨‹
    if cache_dir is not None and os.path.exists(os.path.join(cache_dir, model_name)):
        model_path = os.path.join(cache_dir, model_name)
    else:
        model_path = model_name
        
    max_mem = {i: "15GiB" for i in range(torch.cuda.device_count())}  # ç»™æ¯å¡ç•™ ~1GiB ä½™é‡
    model = AutoModelForCausalLM.from_pretrained(model_path, cache_dir=cache_dir, device_map=device, max_memory=max_mem)
    tokenizer = AutoTokenizer.from_pretrained(model_path, cache_dir=cache_dir, use_fast=False)
    model.eval()
    
    # model.to("cpu")
    
    # 2. åŠç²¾åº¦æ”¯æŒï¼ˆåªåœ¨cudaå¯ç”¨ï¼‰
    if half_model_dtype:
        model.half()
    print("Current dtype:", next(model.parameters()).dtype)
    print("Device:", next(model.parameters()).device)
    
    # 3. è®¾ç½®éšæœºç§å­ï¼ˆå¯é€‰ï¼‰
    # set random seed to guarantee reproducibility
    set_random_seed(seed=0)
    
    
    # 4. å¤„ç† pad_token
    if not hasattr(tokenizer, "pad_token") or tokenizer.pad_token is None:
        print("åŸæ¨¡å‹ä¸­æ²¡æœ‰pad_token,éœ€è¦è‡ªå·±æ·»åŠ ã€‚")
        # æ”¯æŒç‰¹æ®Šçš„ WizardMath-70B
        # set the pad_token of pretrained and finetuned tokenizer
        # note that WizardMath-70B-V1.0 adds two tokens {"<pad>": 32000, "[PAD]": 32001} with (32002, 8192) token embedding size
        # therefore, for WizardMath-70B-V1.0, we add one distinct pad_token "<pad>[PAD]" to reshape the token embedding size to (32001, 8192)
        if "WizardMath-70B-V1.0" in model_name:
            smart_tokenizer_and_embedding_resize(
                special_tokens_dict=dict(pad_token="<pad>[PAD]"),
                model=model,
                tokenizer=tokenizer,
            )
        else:
            smart_tokenizer_and_embedding_resize(
                special_tokens_dict=dict(pad_token="[PAD]"),
                model=model,
                tokenizer=tokenizer,
            )
    
    print("tokenizer vocab size:", tokenizer.vocab_size)
    print("model input embedding size:", model.get_input_embeddings().weight.shape)
    print("model lm_head size:", model.lm_head.weight.shape)
    
    print("æ¨¡å‹åŠ è½½å®Œæ¯•ï¼")
    
    return model, tokenizer


def get_gsm8k_samples(n=20, split="test", field="question+answer", use_prompt=True):
    """
    åŠ è½½ gsm8k çš„å‰ n æ¡æ–‡æœ¬æ•°æ®ï¼Œè¿”å›æ–‡æœ¬åˆ—è¡¨ã€‚
    å‚æ•°:
        n: é‡‡æ ·æ•°é‡
        split: 'train' æˆ– 'test'
        field: 'question' æˆ– 'question+answer'
        use_prompt: æ˜¯å¦åŠ æ ¼å¼å‰ç¼€

    è¿”å›:
        samples: List[str]
    """

    # ä¸‹è½½ gsm8k
    ds = load_dataset("gsm8k", "main", split=split)
    samples = []
    for i in range(min(n, len(ds))):
        q = ds[i]['question'].strip()
        a = ds[i]['answer'].strip()
        if field == "question":
            text = q
        elif field == "question+answer":
            if use_prompt ==True:
                # æ¨èæ ¼å¼ï¼Œå¯çµæ´»è°ƒæ•´
                text = f"Question: {q} Answer: {a}"
                # æˆ– text = f"Q: {q}\nA: {a}"
            else:
                text = q + " " + a
        else:
            raise ValueError("field must be 'question' or 'question+answer'")
        samples.append(text)
    return samples
    
    


def get_humaneval_samples(n=20, split="test", field="prompt+solution", use_prompt=True):
    """
    åŠ è½½ HumanEval çš„å‰ n æ¡æ ·æœ¬ï¼Œè¿”å›æ–‡æœ¬åˆ—è¡¨ã€‚
    å‚æ•°:
        n: é‡‡æ ·æ•°é‡
        split: é€šå¸¸åªæœ‰'test'
        field: "prompt" æˆ– "prompt+solution"
        use_prompt: æ˜¯å¦åŠ å‰ç¼€
    è¿”å›:
        samples: List[str]
    """
    ds = load_dataset("openai_humaneval", split=split)
    samples = []
    for i in range(min(n, len(ds))):
        prompt = ds[i]["prompt"].strip()
        solution = ds[i].get("canonical_solution", "").strip()
        if field == "prompt":
            text = prompt
        elif field == "prompt+solution":
            if use_prompt:
                text = f"# Prompt\n{prompt}\n# Solution\n{solution}"
            else:
                text = prompt + "\n" + solution
        else:
            raise ValueError("field must be 'prompt' or 'prompt+solution'")
        samples.append(text)
    return samples



def collect_all_linear_activations(model, tokenizer, sample_texts, batch_size=1, device=None, verbose=True):
    """
    æ”¶é›†æ¨¡å‹æ‰€æœ‰ Linear å±‚çš„è¾“å…¥å¼ é‡ Xï¼Œæ”¯æŒå¤šæ–‡æœ¬è¾“å…¥ï¼ˆè‡ªåŠ¨æ‰¹é‡å¤„ç†ï¼‰ã€‚
    è¿”å›: activations_dict, module_name_map
    activations_dict: {module_name: [X1, X2, ...]}  # æ¯ä¸ªå±‚åå¯¹åº”è¾“å…¥åºåˆ—åˆ—è¡¨
    module_name_map: {module_name: module_object}   # ä¾¿äºåç»­æŸ¥æ‰¾æƒé‡
    """
    if device is None:
        device = next(model.parameters()).device
    model.eval()
    activations_dict = {}
    module_name_map = {}

    # 1. æ³¨å†Œæ‰€æœ‰ Linear å±‚ hook
    hooks = []
    def register_hooks(module, prefix=''):
        for name, child in module.named_children():
            full_name = f"{prefix}.{name}" if prefix else name
            # å¦‚æœæ˜¯ Linear å±‚ï¼Œåˆ™æ³¨å†Œ hook
            if isinstance(child, nn.Linear):
                module_name_map[full_name] = child
                activations_dict[full_name] = []
                def save_input(name):
                    def hook(mod, inp, out):
                        # inp æ˜¯ tupleï¼Œå–ç¬¬ä¸€ä¸ªå…ƒç´ 
                        activations_dict[name].append(inp[0].detach().cpu())
                    return hook
                hooks.append(child.register_forward_hook(save_input(full_name)))
            else:
                register_hooks(child, full_name)
    register_hooks(model)

    # 2. æ‰¹é‡è¾“å…¥æ–‡æœ¬ï¼Œè§¦å‘ forwardï¼Œæ”¶é›†æ¿€æ´»
    # å»ºè®®åˆ† batch è·‘ï¼Œé¿å…æ˜¾å­˜çˆ†ç‚¸
    total = len(sample_texts)
    for idx in range(0, total, batch_size):
        batch_texts = sample_texts[idx: idx+batch_size]
        # è½¬ tensor
        inputs = tokenizer(batch_texts, return_tensors="pt", padding=True, truncation=True, max_length=2048)
        inputs = {k: v.to(device) for k, v in inputs.items()}
        with torch.no_grad():
            model(**inputs)
        if verbose:
            print(f"Processed {min(idx+batch_size, total)} / {total}")

    # 3. ç§»é™¤æ‰€æœ‰ hooksï¼Œé˜²æ­¢å†…å­˜æ³„æ¼
    for h in hooks:
        h.remove()

    return activations_dict, module_name_map





def compute_wanda_metric(model, activations_dict, module_name_map, save_numpy=True):
    """
    è®¡ç®—æ¯ä¸ª Linear å±‚çš„ Wanda æŒ‡æ ‡çŸ©é˜µ: |Wij| * ||Xj||2
        - X_jï¼šæŒ‡ç¬¬ j ä¸ª in_features é€šé“ï¼Œåˆå¹¶æ‰€æœ‰ batch å’Œ seq æ ·æœ¬ï¼Œå†æ±‚ 2 èŒƒæ•°
        - æƒé‡çŸ©é˜µ Wï¼šshape [out_features, in_features]
    è¾“å…¥:
        model: pytorch model
        activations_dict: {module_name: [X1, X2, ...]}  # æ¯ä¸ªX shape = [batch, seq_len, in_features]
        module_name_map: {module_name: module_object}

    è¾“å‡º:
        wanda_metric_dict: {module_name: wanda_matrix}  # æ¯ä¸ª wanda_matrix shape = [out_features, in_features]
    """
    
    wanda_metric_dict = {}

    for name, X_list in activations_dict.items():
        if len(X_list) == 0:
            print(f"[è­¦å‘Š] å±‚ {name} æ²¡æœ‰é‡‡é›†åˆ°ä»»ä½•è¾“å…¥ï¼")
            continue
    
        # X_list é‡Œæ¯ä¸ª X shape: [batch, seq, in_features]ï¼ˆé€šå¸¸ batch=1ï¼Œseq=ä»»æ„ï¼‰
        # # 1. æ‹¼æ¥æ‰€æœ‰é‡‡æ ·ï¼šå¾—åˆ° [N, S, D]
        # X_cat = torch.cat(X_list, dim=0)  # [N, S, D]ï¼ŒNä¸ºæ‰€æœ‰é‡‡é›†batchæ•°é‡
        # # 2. å±•å¹³æˆä¸€ä¸ªå¤§æ ·æœ¬ï¼šåˆå¹¶ batch å’Œ seq ç»´ï¼Œå¾—åˆ° [total_tokens, in_features]
        # X_cat_flat = X_cat.reshape(-1, X_cat.shape[-1])  # [total_tokens, in_features]
        
        # å…ˆ flatten æ¯ä¸ªæ ·æœ¬
        X_flattened_list = [x.reshape(-1, x.shape[-1]) for x in X_list]  # é€‚é…ä¸ç­‰é•¿
        X_cat_flat = torch.cat(X_flattened_list, dim=0)  # [total_tokens, in_features]


        # 3. å¯¹æ¯ä¸ª in_features åˆ—ï¼ˆé€šé“ï¼‰åš 2 èŒƒæ•°ï¼Œshape [in_features]
        X_col_norm = torch.norm(X_cat_flat, p=2, dim=0)  # æ¯åˆ—åˆå¹¶ååš norm
        
        # 4. å–æƒé‡ |W|ï¼Œshape [out_features, in_features]
        W = module_name_map[name].weight.data.detach().cpu().abs()  # [out_features, in_features]
        
        # 5. Wanda æŒ‡æ ‡ï¼ˆé€åˆ—ä¹˜æ³•ï¼‰ï¼š[out_features, in_features]
        wanda_matrix = W * X_col_norm.unsqueeze(0)  # å¹¿æ’­ï¼šæ¯åˆ—éƒ½ä¹˜ norm
    
        if save_numpy:
            wanda_metric_dict[name] = wanda_matrix.numpy()
        else:
            wanda_metric_dict[name] = wanda_matrix
    
        print(f"{name}: W shape {W.shape}, X_col_norm shape {X_col_norm.shape}, Wanda shape {wanda_matrix.shape}")
        
    return wanda_metric_dict



def save_metric_results(save_path, metric_dict):
    """
    ä¿å­˜ wanda æŒ‡æ ‡ç»“æœåˆ° npz æ–‡ä»¶ã€‚
    å‚æ•°:
        save_path: æ–‡ä»¶åï¼Œå¦‚ 'wizardmath13b_wanda_metrics.npz'
        metric_dict: {module_name: wanda_matrix}
    """
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    out_dir = os.path.dirname(save_path)
    if out_dir and not os.path.exists(out_dir):
        os.makedirs(out_dir)

    # ç”¨ np.savez ä¿å­˜æ‰€æœ‰æ¨¡å—
    np.savez(save_path, **metric_dict)
    print(f"ä¿å­˜ Wanda æŒ‡æ ‡åˆ°: {save_path}")




def analyze_and_save_wanda_metrics(
    model_name,
    get_samples_fn,        # æ•°æ®é‡‡æ ·å‡½æ•°
    n_samples=20,
    sample_kwargs=None,    # é‡‡æ ·å‡½æ•°é¢å¤–å‚æ•°dict
    batch_size=1,
    save_folder="activations",
    dataset_name=None,
    # cache_dir=None,
    half_model_dtype=False,
    seed=0,
    verbose=True
):
    """
    ä¸€ç«™å¼æ”¶é›†å¹¶ä¿å­˜ LLM æ‰€æœ‰ Linear å±‚ Wanda æŒ‡æ ‡
    å‚æ•°:
        model_name: æ¨¡å‹åæˆ–è·¯å¾„
        get_samples_fn: é‡‡æ ·å‡½æ•°ï¼Œå¦‚ get_gsm8k_samples
        n_samples: æ ·æœ¬æ•°
        sample_kwargs: é‡‡æ ·å‡½æ•°å…¶å®ƒå‚æ•°ï¼ˆå¦‚ split/field/use_promptï¼‰
        batch_size: å‰å‘ batch å¤§å°
        save_path: è¾“å‡ºæ–‡ä»¶åï¼Œè‡ªåŠ¨ç”¨ model_name ç”Ÿæˆ
        cache_dir: æ¨¡å‹ç¼“å­˜ç›®å½•
        half_model_dtype: æ˜¯å¦ç”¨åŠç²¾åº¦
        seed: éšæœºç§å­
        verbose: æ˜¯å¦æ‰“å°è¿›åº¦
    """
    if sample_kwargs is None:
        sample_kwargs = {}

    # 1. åŠ è½½æ¨¡å‹å’Œ tokenizer
    model, tokenizer = load_model_and_tokenizer(
        model_name, half_model_dtype=half_model_dtype, seed=seed
    )

    # 2. é‡‡æ ·æ–‡æœ¬æ•°æ®
    sample_texts = get_samples_fn(n=n_samples, **sample_kwargs)
    if verbose:
        print(f"é‡‡æ ·æ•°æ®æ¡æ•°: {len(sample_texts)}")

    # 3. æ”¶é›† activations
    activations_dict, module_map = collect_all_linear_activations(
        model, tokenizer, sample_texts, batch_size=batch_size, verbose=verbose
    )

    # 4. è®¡ç®— Wanda æŒ‡æ ‡
    wanda_metric_dict = compute_wanda_metric(model, activations_dict, module_map)

    # 5. è‡ªåŠ¨ç»„ç»‡ä¿å­˜è·¯å¾„
    if dataset_name is None:
        dataset_name = sample_kwargs.get('dataset_name', 'unknown')
    safe_model_name = model_name.replace('/', '_').replace('\\', '_')
    save_dir = os.path.join(save_folder, dataset_name, f"n{n_samples}")
    os.makedirs(save_dir, exist_ok=True)
    save_path = os.path.join(save_dir, f"{safe_model_name}_wanda_metrics.npz")

    save_metric_results(save_path, wanda_metric_dict)


    del model, tokenizer
    gc.collect()
    torch.cuda.empty_cache()

    print(f"å…¨éƒ¨æµç¨‹ç»“æŸï¼å·²ä¿å­˜è‡³ {save_path}")
    return save_path




def main_wanda_activation_analysis():
    # WizardMath-13B-V1.0 + gsm8k
    analyze_and_save_wanda_metrics(
        model_name="WizardMath-13B-V1.0",
        get_samples_fn=get_gsm8k_samples,
        n_samples=64,
        sample_kwargs={'split': 'test', 'field': 'question+answer', 'use_prompt': True},
        batch_size=1,
        save_folder="activations",
        dataset_name="gsm8k"
    )

    # llama-2-13b-code-alpaca + humaneval
    analyze_and_save_wanda_metrics(
        model_name="llama-2-13b-code-alpaca",
        get_samples_fn=get_humaneval_samples,
        n_samples=64,
        sample_kwargs={'split': 'test', 'field': 'prompt+solution', 'use_prompt': True},
        batch_size=1,
        save_folder="activations",
        dataset_name="humaneval"
    )

if __name__ == "__main__":
    main_wanda_activation_analysis()
