import os
# ğŸ” Redirect all Hugging Face-related caches and locks to your personal directory
os.environ["HF_HOME"] = "/data/songyao/.cache/huggingface"
os.environ["TRANSFORMERS_CACHE"] = "/data/songyao/.cache/huggingface/transformers"
os.environ["HF_DATASETS_CACHE"] = "/data/songyao/.cache/huggingface/datasets"
os.environ["HF_HUB_CACHE"] = "/data/songyao/.cache/huggingface/hub"



# sparse_finetune.py
import os, math, json, random
from typing import Dict, Iterable, Tuple, List, Optional
import torch
from torch.utils.data import Dataset, DataLoader
from datasets import load_dataset
import datasets
from transformers import AutoTokenizer, AutoModelForCausalLM, get_linear_schedule_with_warmup
import wandb
from torch.utils.data import ConcatDataset
from tqdm import tqdm

from record_activations import load_model_and_tokenizer
from extract_key_weights import _sanitize
from utils.utils import set_random_seed
import bitsandbytes as bnb


# ---------------- é€å‚æ•°å¹³å‡ merge delta  ----------------
def build_dense_avg_delta(
    base_model_name_or_path: str,
    ft_model_names_or_paths: List[str],
    out_dense_delta_path: str = "dense_merged_delta_avg.pt",
    seed = 0,
) -> Dict[str, torch.Tensor]:
    """
    å¯¹å¤šä¸ª FT æ¨¡å‹çš„ delta åšé€å‚æ•°å¹³å‡ï¼šdelta_avg[k] = mean_i( ft_i[k] - base[k] )
    """
    
    base_model, _ = load_model_and_tokenizer(base_model_name_or_path, half_model_dtype=False, seed=seed)
    base_sd = base_model.state_dict()
    del base_model
    
    ft_sds=[]
    for ft_name in ft_model_names_or_paths:
        ft_model, _ = load_model_and_tokenizer(ft_name, half_model_dtype=False, seed=seed)
        ft_sd = ft_model.state_dict()
        ft_sds.append(ft_sd)
        del ft_model
    
    
    merged_delta: Dict[str, torch.Tensor] = {}
    for k, v_base in base_sd.items():
        vs = []
        for sd in ft_sds:
            v_ft = sd.get(k, None)
            if v_ft is not None and v_ft.shape == v_base.shape:
                vs.append(v_ft)
        if not vs:
            continue
        acc = None
        for v_ft in vs:
            d = (v_ft - v_base)  # ç»Ÿä¸€åˆ° fp32 åšåŠ æ³•æ›´ç¨³
            acc = d if acc is None else acc + d
        merged_delta[k] = (acc / len(vs)).to(v_base.dtype)

    os.makedirs(os.path.dirname(out_dense_delta_path) or ".", exist_ok=True)
    torch.save(merged_delta, out_dense_delta_path)
    print(f"[OK] dense merged delta å·²ä¿å­˜åˆ°: {out_dense_delta_path}")
    return merged_delta
# ------------------------------------------------


# ---------------- extra train layers  ----------------
def _infer_mask_dtype(mask: Dict[str, torch.Tensor]) -> torch.dtype:
    for v in mask.values():
        if torch.is_tensor(v):
            return v.dtype
    return torch.float32  # å…œåº•


def promote_layers_in_mask(
    mask: Dict[str, torch.Tensor],
    model,
    layers_to_force: List[int],
    mask_dtype: Optional[torch.dtype] = None,
    include_patterns: Optional[List[str]] = None,   # ä¸ºç©ºåˆ™æ‰€æœ‰å‚æ•°éƒ½æ”¾å¼€
):
    """
    å°† model.layers.{i}.** ä¸‹çš„å‚æ•° mask ç½®ä¸ºå…¨ 1ï¼ˆæˆ– Trueï¼‰ï¼Œä½¿è¿™äº›å±‚å‚ä¸è®­ç»ƒã€‚
    - mask: ç°æœ‰çš„ name->tensor æ©ç ï¼ˆCPU ä¸Šï¼‰
    - layers_to_force: ä¾‹å¦‚ [0,1,2,3,4,35,36,37,38,39]
    - include_patterns: æƒ³åªæ”¾å¼€æŸäº›å­æ¨¡å—æ—¶ç”¨ï¼Œæ¯”å¦‚ ["self_attn", "mlp"]ï¼Œé»˜è®¤ None è¡¨ç¤ºè¯¥å±‚æ‰€æœ‰å‚æ•°
    """
    if mask_dtype is None:
        mask_dtype = _infer_mask_dtype(mask)

    is_bool = (mask_dtype == torch.bool)

    def _need_param(name: str, layers_to_force, include_patterns) -> bool:
        ok = any(f"model.layers.{i}." in name for i in layers_to_force)
        if not ok:
            return False
        if include_patterns is None:
            return True
        return any(pat in name for pat in include_patterns)

    changed, added = 0, 0
    for name, p in model.named_parameters():
        if not _need_param(name, layers_to_force, include_patterns):
            continue
        full = torch.ones_like(p, dtype=mask_dtype, device="cpu")
        if is_bool:
            full = full.bool()
        if name in mask:
            # ä¸åŸ mask åˆå¹¶ï¼ˆå¸ƒå°” OR / æµ®ç‚¹ maxï¼‰
            old = mask[name].to(dtype=mask_dtype, device="cpu")
            if is_bool:
                mask[name] = (old.bool() | full.bool())
            else:
                mask[name] = torch.maximum(old, full)
            changed += 1
        else:
            mask[name] = full
            added += 1

    print(f"[promote_layers_in_mask] è¦†ç›–/åˆå¹¶: {changed} ä¸ªå‚æ•°ï¼Œæ–°å¢: {added} ä¸ªå‚æ•°")
    return mask

# ------------------------------------------------


# ---------------- è¾…åŠ©å‡½æ•° ----------------
def apply_delta_inplace(model: torch.nn.Module, delta: Dict[str, torch.Tensor]):
    sd = model.state_dict()
    with torch.no_grad():
        for k, v in delta.items():
            if k in sd and sd[k].shape == v.shape:
                sd[k].add_(v.to(sd[k].dtype).to(sd[k].device))

# def register_grad_masks(model: torch.nn.Module, mask: Dict[str, torch.Tensor]):
#     """ä¸ºå‚ä¸è®­ç»ƒçš„å‚æ•°æ³¨å†Œæ¢¯åº¦æ©ç ï¼›mask å¼ é‡éœ€ä¸ param.shape ç›¸åŒï¼ˆbool/floatå‡å¯ï¼‰"""
#     for name, p in model.named_parameters():
#         if not p.requires_grad:
#             continue
#         m = mask.get(name, None)
#         if m is None:
#             # æ²¡æœ‰æ˜¾å¼ mask çš„å‚æ•°ï¼šä¿æŒæ¢¯åº¦ä¸º 0ï¼ˆç­‰ä»·äºä¸è®­ç»ƒï¼‰
#             p.register_hook(lambda g: torch.zeros_like(g))
#         else:
#             m = m.to(dtype=p.dtype, device=p.device)
#             p.register_hook(lambda g, m=m: g * m)
            
#             # def make_hook(m):
#             #     def hook(g):
#             #         return g * m
#             #     return hook
#             # p.register_hook(make_hook(m))


def apply_grad_masks_step_(model: torch.nn.Module, mask: Dict[str, torch.Tensor]):
    """
    åœ¨ optimizer.step() ä¹‹å‰è°ƒç”¨ï¼š
    æŠŠæ¯ä¸ª param.grad ä¹˜ä»¥å¯¹åº” maskï¼ˆmask å¸¸é©» CPUï¼›æ­¤å¤„ä¸´æ—¶æ¬åˆ° grad.deviceï¼Œç”¨å®Œå³ä¸¢ï¼‰
    å¯¹æ²¡æœ‰ mask æˆ– mask å…¨ 0 çš„å‚æ•°ï¼Œå°† grad æ¸…é›¶ï¼Œä¿è¯å®ƒä»¬ä¸æ›´æ–°ã€‚
    """
    for name, p in model.named_parameters():
        if p.grad is None:
            continue
        m = mask.get(name, None)
        if m is None:
            # æ²¡æœ‰æ˜¾å¼ maskï¼šä¸è®­ç»ƒ
            p.grad.detach().zero_()
            continue
        # å…¼å®¹é bool å­˜å‚¨
        if m.dtype != torch.bool:
            m = (m != 0)
        if not torch.any(m):
            p.grad.detach().zero_()
            continue
        # ä¸´æ—¶æ¬åˆ°æ¢¯åº¦æ‰€åœ¨è®¾å¤‡
        gm = m.to(device=p.grad.device, dtype=p.grad.dtype, non_blocking=True)
        p.grad.mul_(gm)   # åŸåœ°æ©ç 
        # gm ä¼šåœ¨æœ¬æ¬¡å¾ªç¯ç»“æŸåè¢«é‡Šæ”¾ï¼Œä¸å¸¸é©»æ˜¾å­˜


# ---------------- æ•°æ®é›†å°è£…ï¼ˆMathï¼‰ ----------------
class MathSFTDataset(Dataset):
    def __init__(self, tokenizer, max_len=2048, gsm8k_n=None, math_n=None):
        self.tok = tokenizer
        self.samples = []

        # GSM8Kï¼ˆtrainï¼‰
        ds_gsm = load_dataset("openai/gsm8k", "main", split="train")  # Q & Aï¼ˆå¸¦æ­¥éª¤ï¼‰
        if gsm8k_n: ds_gsm = ds_gsm.select(range(min(gsm8k_n, len(ds_gsm))))
        # ds_gsm = load_gsm8k_train(max_n=gsm8k_n, subset="main")
        for ex in ds_gsm:
            q = ex["question"].strip()
            a = ex["answer"].strip()
            prompt = f"Question: {q}\n\nAnswer (step-by-step):\n"
            target = a
            self.samples.append((prompt, target))

        # MATHï¼ˆtrainï¼‰
        ds_math = load_dataset("nlile/hendrycks-MATH-benchmark", split="train")
        if math_n: ds_math = ds_math.select(range(min(math_n, len(ds_math))))
        for ex in ds_math:
            q = ex["problem"].strip()
            sol = ex["solution"].strip()
            prompt = f"Question: {q}\n\nAnswer (step-by-step):\n"
            target = sol
            self.samples.append((prompt, target))

        random.shuffle(self.samples)
        self.max_len = max_len

    def __len__(self): return len(self.samples)

    def __getitem__(self, i):
        prompt, target = self.samples[i]
        # åªå¯¹ target è®¡ lossï¼šprompt éƒ¨åˆ† label = -100
        text = prompt + target
        enc_all = self.tok(text, return_tensors="pt", truncation=True, max_length=self.max_len)
        enc_prompt = self.tok(prompt, return_tensors="pt", truncation=True, max_length=self.max_len)
        input_ids = enc_all.input_ids[0]
        labels = input_ids.clone()
        labels[:enc_prompt.input_ids.shape[1]] = -100
        return {"input_ids": input_ids, "labels": labels, "attention_mask": enc_all.attention_mask[0]}


# ---------------- æ•°æ®é›†å°è£…ï¼ˆCode Alpacaï¼‰ ----------------
class CodeAlpacaDataset(Dataset):
    def __init__(self, tokenizer, max_len=2048, n=None):
        self.tok = tokenizer
        ds = load_dataset("theblackcat102/evol-codealpaca-v1", split="train")
        if n: ds = ds.select(range(min(n, len(ds))))
        self.samples = []
        for ex in ds:
            instr = (ex.get("instruction") or "").strip()
            inp   = (ex.get("input") or "").strip()
            out   = (ex.get("output") or "").strip()
            prompt = f"### Instruction:\n{instr}\n### Input:\n{inp}\n### Response:\n"
            target = out
            self.samples.append((prompt, target))
        self.max_len = max_len

    def __len__(self): return len(self.samples)

    def __getitem__(self, i):
        prompt, target = self.samples[i]
        text = prompt + target
        enc_all = self.tok(text, return_tensors="pt", truncation=True, max_length=self.max_len)
        enc_prompt = self.tok(prompt, return_tensors="pt", truncation=True, max_length=self.max_len)
        input_ids = enc_all.input_ids[0]
        labels = input_ids.clone()
        labels[:enc_prompt.input_ids.shape[1]] = -100
        return {"input_ids": input_ids, "labels": labels, "attention_mask": enc_all.attention_mask[0]}



# ---------------- æ•°æ®é›†å°è£…ï¼ˆEvol-Instruct V2ï¼‰ ----------------
class EvolInstructV2Dataset(Dataset):
    """
    é»˜è®¤åŠ è½½ WizardLMTeam/WizardLM_evol_instruct_V2_196k
    å…¼å®¹å­—æ®µåï¼šinstruction / input / output|response|answer
    è®­ç»ƒæ—¶åªå¯¹ Response æ®µè®¡ lossï¼ˆPrompt æ®µ label = -100ï¼‰
    """
    def __init__(self, tokenizer, dataset_name="WizardLMTeam/WizardLM_evol_instruct_V2_196k",
                 split="train", max_len=2048, n=None):
        self.tok = tokenizer
        ds = load_dataset(dataset_name, split=split)
        if n: ds = ds.select(range(min(n, len(ds))))
        self.samples = []

        for ex in ds:
            instr = (ex.get("instruction") or "").strip()
            inp   = (ex.get("input") or "").strip()
            # ä¸åŒç‰ˆæœ¬å­—æ®µåå¯èƒ½ä¸åŒï¼Œåšä¸ªå…œåº•
            out   = (ex.get("output") or ex.get("response") or ex.get("answer") or "").strip()

            # Alpaca/WizardLM å¸¸è§æ ¼å¼
            prompt = f"### Instruction:\n{instr}\n"
            if len(inp) > 0:
                prompt += f"### Input:\n{inp}\n"
            prompt += "### Response:\n"

            target = out
            if target == "":   # ç©ºæ ·æœ¬è·³è¿‡
                continue
            self.samples.append((prompt, target))

        self.max_len = max_len

    def __len__(self): return len(self.samples)

    def __getitem__(self, i):
        prompt, target = self.samples[i]
        text = prompt + target
        enc_all    = self.tok(text,   return_tensors="pt", truncation=True, max_length=self.max_len)
        enc_prompt = self.tok(prompt, return_tensors="pt", truncation=True, max_length=self.max_len)
        input_ids = enc_all.input_ids[0]
        labels    = input_ids.clone()
        labels[:enc_prompt.input_ids.shape[1]] = -100  # åªè®­ç»ƒ response
        return {"input_ids": input_ids, "labels": labels, "attention_mask": enc_all.attention_mask[0]}


def collate(batch, pad_id):
    ids = [b["input_ids"] for b in batch]
    ams = [b["attention_mask"] for b in batch]
    lbs = [b["labels"] for b in batch]
    maxlen = max(x.size(0) for x in ids)
    def pad_stack(xs, val=0):
        out = []
        for x in xs:
            if x.size(0) < maxlen:
                pad = torch.full((maxlen - x.size(0),), val, dtype=x.dtype)
                x = torch.cat([x, pad], dim=0)
            out.append(x)
        return torch.stack(out, dim=0)
    return {
        "input_ids": pad_stack(ids, pad_id),
        "attention_mask": pad_stack(ams, 0),
        "labels": pad_stack(lbs, -100),
    }
    
    
    
def sparse_finetune(
    base_model_name_or_path: str,
    # --- ç¨€ç–è·¯å¾„ï¼ˆå¯¹äºsparse mergeå’Œtrainï¼‰ ---
    merged_sparse_delta_path: str = "dummy",
    merged_mask_path: str = "dummy",
    # --- æ–°å¢ï¼šå…¨å‚è®­ç»ƒ(dense) ---
    train_all_weights: bool = False,                       # â† True èµ°å…¨å‚
    dense_merged_delta_path: str = None,           # â† å¹³å‡åçš„ dense delta è·¯å¾„
    # --- å…¶å®ƒåŸæœ‰å‚æ•° ---
    out_dir: str = "dummy",
    task_mix: dict = {"math": 1.0, "code": 1.0, "general": 0.0},   # â† æ–°å¢ general
    math_sizes=(8000, 8000),
    code_n=50000,
    general_n=30000,                                               # â† æ–°å¢ï¼šEvolV2 é‡‡æ ·æ•°
    # evol_ds_name="WizardLMTeam/WizardLM_evol_instruct_V2_196k",    # â† æ–°å¢ï¼šæ•°æ®é›†å
    lr=1e-5,
    epochs=1,
    batch_size=1,
    grad_accum=16,
    # bf16=True,
    wandb_proj="sparse-ft",
    run_name="sparse_ft_run",
    seed=42,
    # å¯é€‰ï¼šç»™å…¨å‚/ç¨€ç–ä¸åŒçš„ weight_decay
    wd_sparse: float = 0.0, wd_dense: float = 0.01,
    # â†“â†“â†“ æ–°å¢ï¼šæƒ³é¢å¤–æ”¾å¼€çš„å±‚ç´¢å¼•
    extra_train_layers: Optional[List[int]] = None,
    extra_patterns: Optional[List[str]] = None,  # ä¾‹å¦‚åªæ”¾å¼€ ["self_attn","mlp"]
):
    set_random_seed(seed)
    # 1) åŠ è½½æ¨¡å‹
    print("load")
    model, tok = load_model_and_tokenizer(base_model_name_or_path, half_model_dtype=False, seed=seed, device="balanced_low_0")
    print("load successfully")
    model.train()
    
    # 2) åº”ç”¨ merged ç¨€ç– delta, æˆ–è€…å…¨å‚æ•°delta
    if train_all_weights:
        assert dense_merged_delta_path is not None, "train_all=True éœ€æä¾› dense_merged_delta_path"
        dense_delta = torch.load(dense_merged_delta_path, map_location="cpu")
        apply_delta_inplace(model, dense_delta)
    else:
        assert merged_sparse_delta_path is not None and merged_mask_path is not None, \
            "ç¨€ç–è®­ç»ƒå¿…é¡»æä¾› merged_sparse_delta_path å’Œ merged_mask_path"
        merged_delta = torch.load(merged_sparse_delta_path, map_location="cpu")
        apply_delta_inplace(model, merged_delta)
        
        # 3) æ³¨å†Œæ¢¯åº¦æ©ç 
        merged_mask = torch.load(merged_mask_path, map_location="cpu")
        # register_grad_masks(model, merged_mask)
        
        # â˜… å°†æŒ‡å®šå±‚çš„ mask ç½®ä¸ºå…¨ 1
        if extra_train_layers:
            merged_mask = promote_layers_in_mask(
                merged_mask, model,
                layers_to_force=extra_train_layers,
                mask_dtype=None,                 # è‡ªåŠ¨è·Ÿç°æœ‰ mask çš„ dtype
                include_patterns=extra_patterns  # None=æ•´å±‚ï¼Œæˆ– ["self_attn","mlp"]
            )
    
    
    
    
    # 4) æ•°æ®
    datasets: List[Dataset] = []
    if task_mix.get("math", 0) > 0:
        ds_math = MathSFTDataset(tok, max_len=1024, gsm8k_n=math_sizes[0], math_n=math_sizes[1])
        datasets.append(ds_math)

    if task_mix.get("code", 0) > 0:
        ds_code = CodeAlpacaDataset(tok, max_len=1024, n=code_n)
        datasets.append(ds_code)

    # æ–°å¢ï¼šEvol-Instruct V2ï¼ˆé€šç”¨æŒ‡ä»¤ï¼‰
    if task_mix.get("general", 0) > 0:
        ds_gen = EvolInstructV2Dataset(tok, max_len=1024, n=general_n)
        datasets.append(ds_gen)
        
    
    # ç®€å•æ‹¼æ¥ï¼ˆéœ€è¦æ›´ç²¾ç»†é‡‡æ ·æ—¶å¯å†™ WeightedRandomSamplerï¼‰
    train_ds = ConcatDataset(datasets)
    loader = DataLoader(
        train_ds, batch_size=batch_size, shuffle=True,
        collate_fn=lambda b: collate(b, pad_id=tok.pad_token_id)
    )
    
    
    # 5) ä¼˜åŒ–å™¨ & è°ƒåº¦
    if train_all_weights:
        weight_decay = wd_dense
    else:
       weight_decay = wd_sparse
    # opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.0)
    opt = bnb.optim.AdamW8bit(model.parameters(), lr=lr, weight_decay=weight_decay)
    steps_per_epoch = math.ceil(len(train_ds) / (batch_size * grad_accum))
    num_steps = steps_per_epoch * epochs
    sch = get_linear_schedule_with_warmup(opt, int(0.03 * num_steps), num_steps)
    
    # 6) W&B
    if wandb_proj:
        wandb.init(project=wandb_proj, name=run_name, config={
            "base": base_model_name_or_path,
            "lr": lr, "epochs": epochs, "batch_size": batch_size,
            "grad_accum": grad_accum,
            "math_sizes": math_sizes, "code_n": code_n, "general_n": general_n,
        })
        
    # 7) è®­ç»ƒ
    global_step = 0
    for epoch in range(epochs):
        running = 0.0
        for i, batch in enumerate(tqdm(loader, desc=f"Epoch {epoch+1}")):
            out = model(
                    input_ids=batch["input_ids"],
                    attention_mask=batch["attention_mask"],
                    labels=batch["labels"],
                )
            loss = out.loss / grad_accum
            loss.backward()
            running += loss.item()
            
            if (i + 1) % grad_accum == 0:
                if train_all_weights == False:
                    # åœ¨ step å‰åšä¸€æ¬¡â€œæ­¥å‰æ¢¯åº¦æ©ç â€ï¼Œé¿å… mask å¸¸é©» GPU
                    apply_grad_masks_step_(model, merged_mask)  # ç¨€ç–æ‰éœ€è¦
                
                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                opt.step()
                opt.zero_grad()
                sch.step()
                global_step += 1
                if wandb_proj and global_step % 10 == 0:
                    wandb.log({"loss": running, "lr": sch.get_last_lr()[0], "step": global_step})
                running = 0.0

        print(f"Epoch {epoch+1} done.")    
        
    # 8) ä¿å­˜ HF ç›®å½•ï¼ˆå¯è¢« vLLM/Transformers ç›´æ¥åŠ è½½ï¼‰
    os.makedirs(out_dir, exist_ok=True)
    model.save_pretrained(out_dir)
    tok.save_pretrained(out_dir)
    print(f"[OK] Saved to: {out_dir}")
    if wandb_proj: 
        wandb.finish()
            
            
            

def main():
    
    # # sparse finetune
    # sparse_finetune(
    #     base_model_name_or_path="Llama-2-13b-hf",
    #     merged_sparse_delta_path="sparse_merge_outputs/Llama-2-13b-hf_f53da205/merged_sparse_delta_union_avg_top10.pt",
    #     merged_mask_path="sparse_merge_outputs/Llama-2-13b-hf_f53da205/merged_mask_union_top10.pt",
    #     out_dir="sparse_ft_ckpts/llama2-13b-math-code-alignment-sparseft",
    #     task_mix={"math":1.0, "code":1.0, "general": 1.0},   # åšæ•°å­¦+ä»£ç +å¯¹é½
    #     math_sizes=(8000, 8000),             # GSM8K 8k + MATH 8kï¼ˆå¯æŒ‰æ˜¾å­˜è°ƒå°ï¼‰
    #     code_n=8000,                        # CodeAlpaca å­é›†
    #     general_n=8000,
    #     lr=1e-5, epochs=1, batch_size=1, grad_accum=16,
    #     wandb_proj="weight-sparse-ft", run_name="llama2-13b-sparseft"
    #     )
    
    # # densely all weights finetune
    # build_dense_avg_delta(
    #     base_model_name_or_path = "Llama-2-13b-hf",
    #     ft_model_names_or_paths = ["WizardLM-13B-V1.2", "llama-2-13b-code-alpaca", "WizardMath-13B-V1.0"],
    #     out_dense_delta_path = "dense_merge_outputs/llama2_13b_avg_three.pt",
    #     seed = 42,
    #     )
    # sparse_finetune(
    #     base_model_name_or_path="Llama-2-13b-hf",
    #     train_all_weights=True,
    #     dense_merged_delta_path="dense_merge_outputs/llama2_13b_avg_three.pt",
    #     # ç¨€ç–ç›¸å…³å‚æ•°å¯ä¸ä¼ 
    #     out_dir="dense_ft_ckpts/llama2-13b-math-code-alignment-denseft",
    #     task_mix={"math":1.0, "code":1.0, "general":1.0},
    #     math_sizes=(8000, 8000), code_n=8000, general_n=8000,
    #     lr=1e-5, epochs=1, batch_size=1, grad_accum=16,
    #     wd_dense=0.01,
    #     wandb_proj="wanda-dense-ft", run_name="llama2-13b-denseft",
    # )
    
    
    # sparse finetune with extra_train_layers
    sparse_finetune(
        base_model_name_or_path="Llama-2-13b-hf",
        merged_sparse_delta_path="sparse_merge_outputs/Llama-2-13b-hf_f53da205/merged_sparse_delta_union_avg_top10.pt",
        merged_mask_path="sparse_merge_outputs/Llama-2-13b-hf_f53da205/merged_mask_union_top10.pt",
        out_dir="sparse_ft_ckpts/llama2-13b-math-code-alignment-estra_train_layers_[0,1,2,3,4,35,36,37,38,39]-sparseft",
        task_mix={"math":1.0, "code":1.0, "general": 1.0},   # åšæ•°å­¦+ä»£ç +å¯¹é½
        math_sizes=(8000, 8000),             # GSM8K 8k + MATH 8kï¼ˆå¯æŒ‰æ˜¾å­˜è°ƒå°ï¼‰
        code_n=8000,                        # CodeAlpaca å­é›†
        general_n=8000,  25steps
        lr=1e-5, epochs=1, batch_size=1, grad_accum=16,
        wandb_proj="weight-sparse-ft", run_name="llama2-13b-sparseft",
        # è®© [0..4] ä¸ [35..39] å±‚å…¨éƒ¨å‚ä¸è®­ç»ƒï¼š
        extra_train_layers=[0,1,2,3,4,35,36,37,38,39],
        # å¦‚æœåªæƒ³æ”¾å¼€æ³¨æ„åŠ›å’Œ MLPï¼Œä¸åŠ¨å±‚å½’ä¸€åŒ–/åµŒå…¥ï¼š
        # extra_patterns=["self_attn","mlp"],
        )
    
    
if __name__ == "__main__":
    main()