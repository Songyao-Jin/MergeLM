#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æŒ‰å±‚æå–å…³é”®æƒé‡ï¼ˆTop-r% åˆ° Top-l% åŒºé—´ï¼‰ï¼Œè¾“å‡º mask å­—å…¸ã€‚
ç”¨æ³•ç¤ºä¾‹ï¼š
python extract_key_weights.py \
  --ref_model_path base_state_dict.pth \
  --trained_model_path finetuned_state_dict.pth \
  --out_mask_path mask_top5_10.pt \
  --l_per 5 --r_per 10 \
  --device cpu  # æˆ– cuda:0

è¯´æ˜ï¼š
- ä»…å½“ r_per >= l_per ä¸”äºŒè€…åœ¨ [0,100] å†…æœ‰æ•ˆ
- åŒºé—´å«ä¹‰ï¼šé€‰æ‹©â€œå‰ r% ä»¥å†…ä½†ä¸è¶…è¿‡å‰ l% çš„æƒé‡â€
  ä¾‹ï¼šl=5, r=10 -> é€‰æ‹© 90~95 åˆ†ä½ä¹‹é—´ï¼ˆTop10%~Top5%ï¼‰çš„æƒé‡
"""


import os
# ğŸ” Redirect all Hugging Face-related caches and locks to your personal directory
os.environ["HF_HOME"] = "/data/songyao/.cache/huggingface"
os.environ["TRANSFORMERS_CACHE"] = "/data/songyao/.cache/huggingface/transformers"
os.environ["HF_DATASETS_CACHE"] = "/data/songyao/.cache/huggingface/datasets"
os.environ["HF_HUB_CACHE"] = "/data/songyao/.cache/huggingface/hub"



import math
import argparse
from typing import Dict
import torch
from record_activations import load_model_and_tokenizer



# def load_state_dict(model) -> Dict[str, torch.Tensor]:
#     # ç®€æ´ç‰ˆï¼šå‡è®¾ä¼ å…¥çš„æ˜¯ .pth/.pt/.bin çš„ state_dict
#     # è‹¥ä½ è¦æ”¯æŒ HF ç›®å½•ï¼Œå¯æ”¹æˆ AutoModelForCausalLM.from_pretrained(...).state_dict()
#     # sd = torch.load(path, map_location="cpu")
#     if hasattr(model, "state_dict"):
#         sd = model.state_dict()
#     return sd


@torch.no_grad()
def extract_key_weight_masks(
    ref_sd: Dict[str, torch.Tensor],
    ft_sd: Dict[str, torch.Tensor],
    l_per: float,
    r_per: float,
    device: str = "cpu",
    include_bias: bool = False,
    include_norm: bool = True,
    include_embed: bool = True,
    only_weight_params: bool = True,
    mask_dtype: torch.dtype = torch.bfloat16,  # åç»­ç¨€ç–è®­ç»ƒå¸¸ç”¨ 0/1 bfloat16
):
    """
    è¿”å› mask_dict: {name: Tensor(0/1, same shape)}ï¼Œå¹¶ç»Ÿè®¡è¦†ç›–ç‡
    """
    assert 0 <= l_per <= 100 and 0 <= r_per <= 100, "percent å¿…é¡»åœ¨ [0,100]"
    assert r_per >= l_per, "éœ€è¦æ»¡è¶³ r_per >= l_perï¼ˆTop-r% åˆ° Top-l%ï¼‰"

    mask_dict = {}
    total_elems = 0
    total_masked = 0

    # ç»Ÿä¸€éå† ft_sd çš„é”®ï¼ˆåªå¤„ç†ä¸¤è¾¹éƒ½å­˜åœ¨ä¸” shape ä¸€è‡´çš„ï¼‰
    keys = [k for k in ft_sd.keys() if (k in ref_sd and ft_sd[k].shape == ref_sd[k].shape)]

    for k in keys:
        
        print("The weight matrix we deal with is: ",k)
        
        # å¯é€‰ï¼šä»…å¤„ç† .weight
        if only_weight_params and (not k.endswith(".weight")):
            continue

        # è¿‡æ»¤ norm / embed / biasï¼ˆæŒ‰éœ€æ”¹ï¼‰
        lk = k.lower()
        if (not include_bias) and lk.endswith(".bias"):
            continue
        if (not include_norm) and ("layernorm" in lk or "rmsnorm" in lk or ".norm" in lk):
            continue
        if (not include_embed) and ("embed" in lk):
            continue

        base = ref_sd[k]
        ft = ft_sd[k]
        if not torch.is_floating_point(base) or not torch.is_floating_point(ft):
            print(f"{k} is not float.")
            continue  # è·³è¿‡éæµ®ç‚¹å‚æ•°

        # æœ¬å‚æ•°çš„ |Î”|
        delta_abs = (ft - base).abs().to(device)

        n = delta_abs.numel()
        if n == 0:
            continue

        # éœ€è¦çš„æ•°é‡ï¼ˆå‘ä¸Šå–æ•´ä¿è¯è‡³å°‘å–åˆ°ä¸€ä¸ªï¼‰
        k_high = int(math.ceil(n * (r_per / 100.0)))  # Top-r%
        k_low  = int(math.ceil(n * (l_per / 100.0)))  # Top-l%

        # å°† 1D è§†å›¾æ‹¿æ¥åš k-thï¼ˆkthvalue è¿”å›ç¬¬ k å°ï¼›æˆ‘ä»¬è¦â€œç¬¬ k å¤§â€çš„é˜ˆå€¼ï¼‰
        flat = delta_abs.view(-1)

        # è¾¹ç•Œå¤„ç†ï¼š
        # - r_per == 0ï¼šä¸é€‰ä»»ä½•ï¼ˆé˜ˆå€¼è®¾ä¸º +infï¼Œmask å…¨ 0ï¼‰
        # - l_per == 0ï¼šä¸Šç•Œé˜ˆå€¼ = +infï¼ˆè¡¨ç¤ºä¸æ’é™¤æœ€é¡¶å±‚çš„éƒ¨åˆ†ï¼‰
        INF = torch.tensor(float("inf"), device=device)

        if r_per == 0:
            thr_high = INF  # ä½¿ (val >= thr_high) æ’ä¸º False
        else:
            # ç¬¬ k_high å¤§ <=> ç¬¬ (n - k_high + 1) å°
            idx_small = max(1, n - k_high + 1)
            thr_high = flat.kthvalue(idx_small).values  # æ ‡é‡ã€‚#è¿›å…¥ Top-r% çš„æœ€å°å€¼é—¨æ§›ã€‚

        if l_per == 0:
            thr_low = INF  # ä¸æ’é™¤ Top-l%ï¼ˆå› ä¸º l=0ï¼‰
        else:
            idx_small = max(1, n - k_low + 1)
            thr_low = flat.kthvalue(idx_small).values   #è¿›å…¥ Top-l% çš„æœ€å°å€¼é—¨æ§›ã€‚

        # ç›®æ ‡åŒºé—´ï¼š >= thr_high ä¸” < thr_low
        # ä¾‹ï¼šl=5%, r=10% -> é€‰ Top10% åˆ° Top5% ä¹‹é—´
        if r_per == 0 or delta_abs.max() == 0:
            local_mask = torch.zeros_like(delta_abs, dtype=torch.bool, device=device)
        else:
            if thr_high == 0:
                cond_high = delta_abs > thr_high
            else:
                cond_high = delta_abs >= thr_high
            if l_per > 0:
                cond_low_exclude = delta_abs >= thr_low  # è¿™äº›å±äº Top-l%ï¼Œéœ€è¦æ’é™¤
                local_mask = cond_high & (~cond_low_exclude)
            else:
                local_mask = cond_high

        # ç»Ÿè®¡ & å­˜å‚¨
        total_elems += n
        total_masked += local_mask.sum().item()

        # ä¿å­˜ä¸ºæŒ‡å®š dtypeï¼ˆ0/1ï¼‰
        mask_val = local_mask.to(mask_dtype)
        # å› CPU å­˜ç›˜æ›´é€šç”¨
        mask_dict[k] = mask_val.cpu()

    ratio = 100.0 * (total_masked / max(1, total_elems))
    return mask_dict, ratio



def _sanitize(name: str) -> str:
    """æŠŠæ¨¡å‹åé‡Œçš„ / \ ç©ºæ ¼ ç­‰æ›¿æ¢æˆ _ï¼Œç”¨äºå®‰å…¨çš„æ–‡ä»¶å/ç›®å½•åã€‚"""
    return name.replace("/", "_").replace("\\", "_").replace(" ", "_")

def build_out_mask_path(
    base_model_name: str,
    ft_model_name: str,
    l_per: float,
    r_per: float,
    include_bias: bool,
    include_norm: bool,
    include_embed: bool,
    only_weight_params: bool,
    mask_dtype: str = "bfloat16",
    root_dir: str = "masks"
) -> str:
    """
    æ ¹æ®æ¨¡å‹åä¸ç­›é€‰å‚æ•°è‡ªåŠ¨ç”Ÿæˆè¾“å‡ºè·¯å¾„:
      masks/{base}__{ft}/top{r}_to_top{l}__flags__{dtype}.pt
    ä¾‹å¦‚:
      masks/meta-llama_Llama-2-13b-hf__WizardLM_WizardMath-13B-V1.0/top10_to_top0__bias-norm-embed__bf16.pt
    """
    b = _sanitize(base_model_name)
    f = _sanitize(ft_model_name)
    flags = []
    if include_bias:  flags.append("bias")
    if include_norm:  flags.append("norm")
    if include_embed: flags.append("embed")
    if only_weight_params: flags.append("onlyW")
    flags_str = "-".join(flags) if flags else "all"
    dtype_short = {"bool":"bool", "float16":"fp16", "bfloat16":"bf16", "float32":"fp32"}[mask_dtype]
    subdir = os.path.join(root_dir, f"{b}_{f}")
    fname = f"top{int(r_per)}_to_top{int(l_per)}_{flags_str}_{dtype_short}.pt"
    return os.path.join(subdir, fname)



def main():
    
    base_model_name = "Llama-2-13b-hf"
    ft_model_name =  "llama-2-13b-code-alpaca"           #"WizardLM-13B-V1.2"        #"llama-2-13b-code-alpaca"           #"WizardMath-13B-V1.0"
    l_per = 0.0               # Top-l%ï¼ˆä¸Šç•Œï¼Œ0 è¡¨ç¤ºä¸æ’é™¤æœ€é¡¶å±‚ï¼‰
    r_per = 10.0              # Top-r%ï¼ˆä¸‹ç•Œï¼‰
    device = "cpu"            # å»ºè®®ç”¨ cpu ä»¥é¿å…å¤šå¡ device mismatch
    include_bias = True
    include_norm = True
    include_embed = True
    only_weight_params = False
    mask_dtype = "bfloat16"   # "bool"|"float16"|"bfloat16"|"float32"
    
    
    dtype_map = {
        "bool": torch.bool,
        "float16": torch.float16,
        "bfloat16": torch.bfloat16,
        "float32": torch.float32,
    }


    out_mask_path = build_out_mask_path(
        base_model_name, ft_model_name, l_per, r_per,
        include_bias, include_norm, include_embed, only_weight_params,
        mask_dtype=mask_dtype, root_dir="masks"
    )

    # ===== åŠ è½½æ¨¡å‹ =====
    print("åŠ è½½ base, finetuned æ¨¡å‹ ...")
    base_model, _ = load_model_and_tokenizer(base_model_name, half_model_dtype=False, seed=0)
    ft_model, _ = load_model_and_tokenizer(ft_model_name, half_model_dtype=False, seed=0)


    # ===== 2. å– state_dict å¹¶ç»Ÿä¸€æ¬åˆ° CPUï¼Œé¿å… (ft - base) çš„è·¨è®¾å¤‡é—®é¢˜ =====
    print("æŠ½å– state_dict åˆ° CPU ...")
    ref_sd = {k: v.detach().cpu() for k, v in base_model.state_dict().items()}
    ft_sd  = {k: v.detach().cpu() for k, v in ft_model.state_dict().items()}
    

    # ===== 3. æå–å…³é”®æƒé‡ =====
    print(f"å¼€å§‹æŒ‰å±‚æå–å…³é”®æƒé‡: Top-{r_per}% åˆ° Top-{l_per}%")
    mask_dict, ratio = extract_key_weight_masks(
        ref_sd, ft_sd,
        l_per=l_per, r_per=r_per,
        device=device,  # è¿™é‡Œä¾ç„¶ä¼  "cpu" å°±å¥½
        include_bias=include_bias,
        include_norm=include_norm,
        include_embed=include_embed,
        only_weight_params=only_weight_params,
        mask_dtype=dtype_map[mask_dtype],
    )

    # ===== 4. ä¿å­˜ =====
    os.makedirs(os.path.dirname(out_mask_path) or ".", exist_ok=True)
    torch.save(mask_dict, out_mask_path)
    print(f"å·²ä¿å­˜ mask åˆ°: {out_mask_path}")
    print(f"Mask è¦†ç›–ç‡: {ratio:.2f}%")

if __name__ == "__main__":
    main()
